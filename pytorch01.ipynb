{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import feature\n",
    "from skimage import filters\n",
    "from skimage import exposure\n",
    "from skimage import img_as_float\n",
    "from skimage import io\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from utils import mkdir, rmdir\n",
    "from ipywidgets import Output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mkdir('pics')\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v0.20.5 (Git Hash 0125f28c61c1f822fd48570b4c1066f96fcb9b2e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - NNPACK is enabled\n",
      "  - CUDA Runtime 10.1\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 7.6.3\n",
      "  - Magma 2.5.1\n",
      "  - Build settings: BLAS=MKL, BUILD_NAMEDTENSOR=OFF, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Wno-stringop-overflow, DISABLE_NUMA=1, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=True, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*torch.__config__.show().split(\"\\n\"), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 8\n",
    "CLASS_NUM = 43\n",
    "\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "MAX_ROTATION = 30\n",
    "\n",
    "MIN_BRIGHTNESS = 0.8\n",
    "MAX_BRIGHTNESS = 1.2\n",
    "\n",
    "MIN_CONTRAST = 0.8\n",
    "MAX_CONTRAST = 1.2\n",
    "\n",
    "MIN_SATURATION = 0.8\n",
    "MAX_SATURATION = 1.2\n",
    "\n",
    "DISTORTION = 0.1\n",
    "MAX_TRANSITION = 0.25\n",
    "\n",
    "MIN_SCALE = 0.9\n",
    "MAX_SCALE = 1.1\n",
    "\n",
    "NOISE_STD = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomNoise(tensor):\n",
    "    std = np.random.uniform(0, NOISE_STD)\n",
    "    return torch.clamp(tensor + torch.FloatTensor(tensor.size()).normal_(0, std), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = {\n",
    "    'default': T.Compose([\n",
    "        T.Resize(IMG_SIZE),\n",
    "        T.CenterCrop(IMG_SIZE),\n",
    "        T.ToTensor()\n",
    "    ]),\n",
    "    'train': T.Compose([\n",
    "        T.RandomAffine(MAX_ROTATION, (MAX_TRANSITION, MAX_TRANSITION), (MIN_SCALE, MAX_SCALE)),\n",
    "        # T.RandomPerspective(DISTORTION),\n",
    "        T.Resize(IMG_SIZE),\n",
    "        T.CenterCrop(IMG_SIZE),\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(RandomNoise),\n",
    "    ]),\n",
    "    'test': T.Compose([\n",
    "        T.Resize(IMG_SIZE),\n",
    "        T.CenterCrop(IMG_SIZE),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __init__(self, image_size):\n",
    "        self.image_size = image_size\n",
    "        self.image_transform = T.Compose([\n",
    "            T.ColorJitter((MIN_BRIGHTNESS, MAX_BRIGHTNESS),\n",
    "            (MIN_CONTRAST, MAX_CONTRAST), (MIN_SATURATION,MAX_SATURATION)),\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(RandomNoise),\n",
    "        ])\n",
    "        return\n",
    "    \n",
    "    def __call__(self, img, mask, is_test):\n",
    "        w, h = img.size\n",
    "\n",
    "        if not is_test:\n",
    "#             while True:\n",
    "#                 crop_left, crop_right = sorted(np.random.randint(0, w, size=2))\n",
    "#                 if crop_right - crop_left > w * 0.9:\n",
    "#                     break\n",
    "            \n",
    "#             while True:\n",
    "#                 crop_top, crop_bottom = sorted(np.random.randint(0, h, size=2))\n",
    "#                 if crop_bottom - crop_top > h * 0.9:\n",
    "#                     break\n",
    "                    \n",
    "            angle = np.random.uniform(-90, 90)\n",
    "            translate =  tuple(np.random.uniform(-16, 16, size=(2,)))\n",
    "            scale = np.random.uniform(0.9, 1.1)\n",
    "            shear = np.random.uniform(-0, 0)\n",
    "            fillcolor = 255 # np.random.randint(0, 256)\n",
    "            \n",
    "            img = TF.affine(img, angle, translate, scale, shear, fillcolor=fillcolor)\n",
    "            mask = TF.affine(mask, angle, translate, scale, shear, fillcolor=fillcolor)\n",
    "\n",
    "            # img = TF.crop(img, crop_top, crop_left, crop_bottom - crop_top, crop_right - crop_left)\n",
    "            # mask = TF.crop(mask, crop_top, crop_left, crop_bottom - crop_top, crop_right - crop_left)\n",
    "        \n",
    "        img = TF.resize(img, self.image_size, interpolation=2)\n",
    "        mask = np.array(TF.resize(mask, self.image_size, interpolation=2))\n",
    "        \n",
    "        if is_test:\n",
    "            img = TF.to_tensor(img)\n",
    "        else:\n",
    "            img = self.image_transform(img)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train.csv'\n",
    "test_path = 'dataset/test.csv'\n",
    "\n",
    "image_path = 'dataset/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, as_gray=True):\n",
    "    return Image.fromarray(io.imread(path, as_gray=as_gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed827728323453b9473a91c98a8445a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23960.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b853ff1d1964b6b9fa172e90ff92680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23960.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOFklEQVR4nO3d24uW1RfA8e1hPNtBbcaaDFMLibAJcgg7GOaliCH4B3ghSBdCgjdddVESFAleaaI3FuRFRQoJURAUiJRoF3nEszhqjo6n8TTV1W+11vq9z57H8Z2Z9bzv93O13vZ+DzPj6ln73Xs/e8Q///yTAMQzcrg/AIDaSE4gKJITCIrkBIIiOYGgRucae3t7C7/KHTVqlMT+G98RI0bU7Of19fUVvkbR8/7++2/zeOTI//7/ot83pZTu379f2DYQ/r2Lfk79mVKyP1vud6Xp341/Tf8c3de/d9Fr5L6l123+c+j39u9V9Pq53309Zgse5G9b9nMN5P0e4t9YzSdy5QSCIjmBoEbkyoq+vr5SNUeuVCtb0tS7xEjJlqG5zzTUpVUZZctf37fenyMq/XM2wO+KshaoEpITCIrkBILKjjlTSqyKBwYfY06gSkhOICiSEwiK5ASCIjmBoEhOIKjsrpSgS52ApsCVEwiK5ASCypa1lLLA8OHKCQRFcgJBkZxAUCQnEBTJCQRFcgJBsUIIQ67eJ9s16r9NrpxAUCQnEBQrhAIpW+7l+jXS36zs76OKP3OZISNXTiAokhMIiuQEgsqOOTG0yh7L55Udc9V7CqMeBvrZqzjO1Mp8fq6cQFAkJxAUK4SC8mXcYJekRa8/0FOjB/M5jYCpFKDCSE4gKL6tDWqgw4h6D0WGuuxslqGUPnV91KhRNftw5QSCIjmBoEhOICjGnIHo8VZfX59pGzlyZGGbpsdp+jm+zY8l9RhItz3IlI5/v1rv69/r/v37pk2Pv1paWgpfoxlw5QSCIjmBoChrA9Hlqi8fdfnnS8Eit2/fNo+vXbsm8a1btwpfv6enR+Le3l7TT7f5UnPChAkS50pX/TxduqaUUmdnp8SPPfaYxEXTDVXFwnegwkhOICiSEwiKMWdQfkxy48YNiQ8ePGjaDh06JLEeV164cMH0O3r0qMSXLl0qfH09XvTTI6NH//dPxo9br1y5UrNt/Pjxpt+4ceMkfuSRR0zb+vXrJV68eHFqVIw5gQojOYGgKGvrpN67N44dO2Yeb926VeKff/7ZtP31118S37lzR+IpU6aYfo8//njh+02dOlViXWrq6YyUUnr00UclzpW1p0+flthPl+gy95VXXjFtzzzzjMS6pPZlYDOsGOLKCQRFcgJBVaKsLbsQu+ztI3P99DeVuc+Re82B3sZSr8bZtGmTafv6668l9qXm/PnzJdZl4ezZs02/l156qfA1xowZI7H+NtWvzNElqv85dUnd1dUlsV+or9+rvb3dtOlSXL93M5SxHldOICiSEwiK5ASCCnvf2qIxnB8T3rx5U+Lr16+btnv37knc3d1d6r30dEBK9ut8/XoppXT37l2J9XjL/670qprJkyebtra2tpqf/8iRI6bfpEmTJH7iiScK2xYtWiTxa6+9ZvrpsZ6f3tAGekSCHltOmzat1Ov7MW3Rhu1mxG8CCIrkBIKqxFSK5lelbNy4UeI9e/YU9tXlry+ddHnq23TZldvkrEs8XzLqKRI/raDLYf36EydONP10SXrmzBnTduLEiZqft6Ojw/Tzr6nlVuMUKTsl5UtX/bzcfY4aefqE4xiACiM5gaBITiCosGPOovHM5cuXzeNdu3ZJrMdeKdllaHrM5sdAeimbnpZIyS4nGzt2bOHzcmM2Pf46e/asadObo/X48+rVq6afHhfrnyUlO1bdvXu3xG+88Ybpt2LFCon19E696N9BbkqkkceSZbHZGqgwkhMIKmxZW8RPZ+iNwTNnzjRtRStilixZYvrplTR+ukE/9lMkumzWZXjuPq26dPX0BuWPPvrItF28eFFiX9bq+8Xqsvnbb781/fTqodbWVtM2kJU5udKs7ObzZtxEXRZXTiAokhMIiuQEgsqOOYez/i+6y8CTTz5p+n3wwQcS+3ux6vGdHmO98MILpp/eKZL7mX1b2SPv9Of3O1v0423btkl88uRJ00+Pb/UNvVKyUyZ6msjf31a/ph9z1sNQH1Hf6LhyAkGRnEBQlZtK0eVdSvamVX7Hh35cdGJySuV3QuR2UOT+e26a5bfffpN4586dEvsVPHrFkN+Zs3TpUonnzJkj8eeff2766bLW3y+2rHoMdSh/y+HKCQRFcgJBhf22VsuVnbr886Vg0X2I/GuUvT/qQDch6+f50nv79u0S69J1+vTppp9eIeRP5po7d67E+rQwL3efo6LPOxiaZUN1DputgQojOYGgSE4gqLD3rS3ipzPK7qYYjJ+l7Fkp+rHeXJ2Snd7Qm7KfffZZ008fCehXOOn72OrN4n4MrjdsD8bfM8q/kSooM53ElRMIiuQEggpb1tb7/aKUXMePHzePz58/L7Eua/3REnpR/9tvv23a9EnRuZ9TL/Bnlc7w4h5CQIWRnEBQJCcQVCWW71WdHkvu37/ftN2+fbvmc/744w/zWN8/Vx8zn5L9OxW9Xkr2SHrGnPFx5QSCIjmBoMJOpTQS/Xvs6ekxbXoVjz5V2+8uWbx4scTPP/984XudOnWq5munZHe6DMZxDKgvrpxAUCQnEBS1zSDIHTGgV+mkZE8u0xux/b2S9BES/sgI/Q2t/ja4s7PT9Gtra5O47P2QMHy4cgJBkZxAUCQnEBRjziGgx3dPP/20adNjUD3OnDdvnun3+uuvS+zHh/rowH379km8evVq08+PYzF82JUCVBjJCQTFwvdBkLtv7VNPPWXapk6dWvM13nnnHfNYL1rXC+lTSumnn36SuLu7W+L29vbCz4H4uHICQZGcQFAkJxAUUymDIHcE4KxZs0ybPrJv9uzZEr/55pum35gxYyTu6uoybd99913N1+/o6DD99JkwGF6clQJUGMkJBEVZWydFRzN4/vi+d999V2Jddra2tpp++kTsHTt2mLaDBw9K/P7770usj2lALKwQAiqM5ASC4h5CQ6ylpcU89gvh/8efgP37779L/MUXX5g2fU+ht956S2L9DW9/+FvHw5UTCIrkBIIiOYGgmEqpk7LHG/hVOkUnYh8+fNj0+/DDDyXW0yoppbR27VqJ9SojP77NjSUZZ8bDlRMIiuQEgqKsHWJ+o7QuZfVRChs2bDD9/vzzT4nfe+8907Zw4UKJ9fQJpWpcrBACKozkBIIiOYGgGHPWSdnxnZ9yuXTpksSbN2+W+JdffjH99A2/VqxYYdr8lAniY7M1UGEkJxAUZe0Q0CXMtWvXTJveYfLNN99IPHfuXNNv5cqVEk+ZMsW06eMecuV1PXae5FZCMXVTX1w5gaBITiAoylrHl225Mq6ozW+UvnXrlsRffvmladuyZYvE06dPl3jNmjWmn76Fpi8fdVlbtJC+P0Ulb9kyub++sFghBFQYyQkERXICQTHm7EduDHfv3r2azzl//rx5/NVXX0m8detW06aPT1i3bp3EnZ2dpp/epO0/h97pkvu8emxa9miGBxm3ojxWCAEVRnICQXGytZMr4/xGaT1lcvr0aYnXr19v+u3evVviSZMmmbYXX3xR4kOHDkmsj1jwz9P3CUrJHrswbdo0iceNG2f6+ccYPpS1QIWRnEBQJCcQ1Ih+xljSqL+Gb2S5aYq7d++atr1790r88ccfS/zDDz+Yfvo+sxMmTDBteoeJfn3/+9bjRX8GyowZMyR+7rnnJPbTMfPnz5d45syZpk3/nPq9x48fn4r439Xo0czMleXGnDUHnc2RcUAFkZxAUJS1jp8u0auAfvzxR9Omj0E4efKkxG1tbaZfUdnp+/b29krs/y5Hjx6V+MKFC6bt5s2bNWM/daKPG1y+fLlp0685ceJEiVetWmX66df0/ybGjh0rcTNOwz0IylqgwkhOIChWCDm+rNWl5q5duwr7Llu2TGL9rWhKKS1atEji9vZ201a0oN1/jp6eHokvX75s2o4cOSLxvn37JPa319Qnl33yySemraurS2JduvrPu3Tp0pr9UH9cOYGgSE4gKJITCCo7lZJSarqdtv7mXNevX5dY7y5Jye4U0WMzP06bPHmyxLkpKT3O9OP93E239POuXr0q8Z49e0y/HTt2SPzrr7+aNr2rRk+JvPzyy6bfp59+KnFHR4dp06uJmvH7iofAVApQJSQnEBRlreOnMPRidH/PIF266SmRBylJdZmrS+rcPX7KlsZ6wX1KKZ09e1bi77//3rRt375dYj1tc+fOHdPv1Vdflfizzz4zba2traU+I1ghBFQayQkERXICQTHmdHJnpUSZHsh9Dj9m1vQY1I9Hjx8/LrE+z+XcuXOmX3d3t8T6PrsppbRgwQKJy94Xt1kx5gQqjOQEgqKsbTBl77ubO1riypUrEh84cMD00+Wqvw+RPqaQqZQ8ylqgwkhOICjK2gYz0LJWr07Sbf52oLnbX+buL4QsylqgSkhOICiSEwiKMWcTyf2ti24ulluN5Demt7S0lHoe/g9jTqBKSE4gKI6FajD9DFNKKbuZG4OLKycQFMkJBEVyAkEx5kRKyU595DaY6/Fo7kZmeHhcOYGgSE4gKMraBuanVXJlZ9EUTG5qhp0nA1fm3lT8doGgSE4gKJITCIpdKUgplV/2x3TJoGBXClAlJCcQFFMpSClRrkbElRMIiuQEgiI5gaBITiAokhMIiuQEgiI5gaBITiAokhMIiuQEgiI5gaBITiAokhMIiuQEgiI5gaBITiAoNlsDw4D71gIVRnICQZGcQFAkJxAUyQkERXICQZGcQFAkJxAUyQkERXICQZGcQFAkJxAUyQkExa4UYBiUOXKRKycQFMkJBNVfWctxx8Aw4coJBEVyAkGRnEBQJCcQFMkJBEVyAkH9C6Ab6+ckmIxcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, path, image_path, \n",
    "                 target='classify',\n",
    "                 transform=my_transforms['default'],\n",
    "                 is_train=True, verbose=True):        \n",
    "        self.is_train = is_train\n",
    "        # if not isinstance(transform, tuple):\n",
    "        #     self.transform = (transform, transform)\n",
    "        # else:\n",
    "        #     self.transform = transform    \n",
    "        # self.transform = transform\n",
    "        self.transform = transform\n",
    "        \n",
    "        assert target in {'reconstruct', 'classify'}\n",
    "        self.target = target\n",
    "        \n",
    "        # reading labels\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        df = df[(df.vowel_diacritic == 0) & (df.consonant_diacritic == 0)]\n",
    "        \n",
    "        self.image_ids = df.image_id.unique()\n",
    "        self.length = len(self.image_ids)\n",
    "        \n",
    "        if self.is_train:\n",
    "            self.labels = df[['grapheme_root','vowel_diacritic','consonant_diacritic']].values\n",
    "        else:\n",
    "            self.labels = [None] * self.length\n",
    "        \n",
    "        # reading images\n",
    "        iterator = [os.path.join(image_path, f'{x}.png') for x in self.image_ids]\n",
    "        with mp.Pool(16) as pool:\n",
    "            iterator = pool.imap(load_image, iterator)\n",
    "            if verbose:\n",
    "                iterator = tqdm(iterator, total=len(self.image_ids))\n",
    "            self.images = list(iterator)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.target == 'reconstruct':            \n",
    "            img = self.images[idx]\n",
    "            img1, img2 = self.transform(img, img, not self.is_train)\n",
    "            return img1, img2\n",
    "            # img = self.transform(img)\n",
    "            # return img, img\n",
    "        return self.transform(self.images[idx]), self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "Autoencoder_Train = BengaliDataset(train_path, image_path, target='reconstruct',\n",
    "    transform=MyTransform((64,64)),\n",
    "    # transform=my_transforms['default'],\n",
    "    is_train=True, verbose=True)\n",
    "Autoencoder_Test = BengaliDataset(train_path, image_path, target='reconstruct',\n",
    "    transform=MyTransform((64,64)),\n",
    "    is_train=False, verbose=True)\n",
    "\n",
    "for idx in range(1):\n",
    "    img, target = Autoencoder_Test[idx]\n",
    "    \n",
    "    plt.imshow(TF.to_pil_image(img), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(TF.to_pil_image(target), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.residuals = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        y = self.pool(y)\n",
    "        x = self.residuals(x)\n",
    "        x = self.pool(x)\n",
    "        return x + y\n",
    "        \n",
    "\n",
    "# x = torch.FloatTensor(size=(1,1,64,64))\n",
    "# test_encoder_layer = EncoderLayer(1, 16)\n",
    "# test_encoder_layer(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLayer(\n",
      "  (conv_transpose): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (layers): Sequential(\n",
      "    (0): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upsample): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "  (residuals): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 8, 8])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_ch, in_ch, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01,inplace=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2.0)\n",
    "        self.residuals = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        y = self.conv_transpose(x)\n",
    "        y = self.layers(y)\n",
    "        x = self.upsample(x)\n",
    "        x = self.residuals(x)\n",
    "        return x + y\n",
    "    \n",
    "x = torch.FloatTensor(size=(16,128,4,4))\n",
    "test_decoder_layer = DecoderLayer(128, 64)\n",
    "print(test_decoder_layer)\n",
    "test_decoder_layer(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([16, 1, 64, 64])\n",
      "Encoded: torch.Size([16, 64]) torch.Size([16, 64])\n",
      "Decoded: torch.Size([16, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, code_shape=32, filters=16, input_shape=(64,64)):\n",
    "        super(VAE, self).__init__()  \n",
    "        \n",
    "        # params\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_shape = (input_shape[0] // 16, input_shape[1] // 16)\n",
    "        self.code_shape = code_shape\n",
    "        self.filters = filters\n",
    "        \n",
    "        # encoder convs\n",
    "        self.input_layer = nn.BatchNorm2d(1)\n",
    "        self.encoder_convs = nn.Sequential(\n",
    "            EncoderLayer(1, filters), # 1x64x64 -> 16x32x32\n",
    "            EncoderLayer(filters, filters * 2), # 16x32x32 -> 32x16x16\n",
    "            EncoderLayer(filters * 2, filters * 4), # 32x16x16 -> 64x8x8\n",
    "            EncoderLayer(filters * 4, filters * 8), # 64x8x8 -> 128x4x4\n",
    "        )\n",
    "        \n",
    "        # encoder dense\n",
    "        num_features = self.filters * 8 * np.prod(self.hidden_shape)\n",
    "        self.encoder_fcn = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(512),\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(512, self.code_shape)\n",
    "        self.logvar_layer = nn.Linear(512, self.code_shape)\n",
    "    \n",
    "        # decoder convs\n",
    "        self.decoder_fcn = nn.Sequential(\n",
    "            nn.Linear(self.code_shape, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, num_features)\n",
    "        )\n",
    "            \n",
    "        self.decoder_convs = nn.Sequential(\n",
    "            DecoderLayer(filters * 8, filters * 4), # 128x4x4 -> 64x8x8\n",
    "            DecoderLayer(filters * 4, filters * 2), # 64x8x8 -> 32x16x16\n",
    "            DecoderLayer(filters * 2, filters), # 32x16x16 -> 16x32x32\n",
    "            DecoderLayer(filters, filters), # 16x32x32 -> 16x64x64\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(filters, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.encoder_convs(x)\n",
    "        x = self.encoder_fcn(x)\n",
    "        \n",
    "        mean = self.mean_layer(x)\n",
    "        logvar = self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparametrize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + std * eps\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder_fcn(x)\n",
    "        x = x.view(-1, self.filters * 8, *self.hidden_shape)\n",
    "        x = self.decoder_convs(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mean, logvar)\n",
    "        return self.decode(z), mean, logvar\n",
    "    \n",
    "####\n",
    "\n",
    "vae = VAE(64, filters=16, input_shape=(64,64))\n",
    "# vae test\n",
    "\n",
    "x = torch.FloatTensor(size=(16, 1, 64, 64))\n",
    "print('Input:', x.size())\n",
    "\n",
    "mean, logvar = vae.encode(x)\n",
    "print('Encoded:', mean.size(), logvar.size())\n",
    "\n",
    "decoded = vae.decode(mean)\n",
    "print('Decoded:', decoded.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.value = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, value, count=1):\n",
    "        value = float(value)\n",
    "        self.value = (self.value * self.count + value * count) / (self.count + count)\n",
    "        self.count += count\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.value:.5f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    if epoch < 64:\n",
    "        return 10\n",
    "    if epoch < 256:\n",
    "        return 1\n",
    "    if epoch < 512:\n",
    "        return 1e-1\n",
    "    if epoch < 1024:\n",
    "        return 1e-2\n",
    "    return 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VAE(32, filters=32, input_shape=(64,64))\n",
    "net = net.to(device)\n",
    "\n",
    "mkdir(rmdir('pics'))\n",
    "image_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 1e-3\n",
    "optimizer = optim.Adamax([\n",
    "    {'params': net.input_layer.parameters(), 'lr': base_lr},\n",
    "    {'params': net.encoder_convs.parameters(), 'lr': base_lr},\n",
    "    {'params': net.encoder_fcn.parameters(), 'lr': base_lr},\n",
    "    {'params': net.mean_layer.parameters(), 'lr': base_lr},\n",
    "    {'params': net.logvar_layer.parameters(), 'lr': base_lr},\n",
    "    {'params': net.decoder_fcn.parameters(), 'lr': base_lr},\n",
    "    {'params': net.decoder_convs.parameters(), 'lr': base_lr},\n",
    "    {'params': net.output_layer.parameters(), 'lr': base_lr},\n",
    "],  betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_kld = 0.001\n",
    "\n",
    "def loss_function(x, x_gt, mean, logvar):\n",
    "    # bce = nn.BCELoss()(x.view(-1, 64*64), x_gt.view(-1, 64*64))\n",
    "    # main = nn.MSELoss(reduction='sum')(x.view(-1, 64*64), x_gt.view(-1, 64*64))\n",
    "    main = torch.pow(x.view(-1, 64*64) - x_gt.view(-1, 64*64), 2).sum(dim=1)\n",
    "    kld = lambda_kld * -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "    return main.sum(), kld.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training settings\n",
    "num_epochs = 768\n",
    "batch_size = 32\n",
    "batch_per_epoch = 512\n",
    "\n",
    "num_test_batches = 3\n",
    "num_sample = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 748\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 20\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = DataLoader(Autoencoder_Train, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=num_workers, drop_last=True)\n",
    "test_loader = DataLoader(Autoencoder_Test, batch_size=test_batch_size, shuffle=False)\n",
    "print('Train len:', len(train_loader))\n",
    "batch_per_epoch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964ac5e92cec4dedb29d6db410c2f703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=574464.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e9d698201c413687420575b839e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3c27d780c54d1383c5a04944f779f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-621-97f7ca18a296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mXpred_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXpred_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXgt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-514-b114cbd241a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparametrize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-514-b114cbd241a1>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_convs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-513-c7974166e01c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=num_epochs * batch_per_epoch)\n",
    "\n",
    "metrics_output = Output()\n",
    "plot_output = Output()\n",
    "\n",
    "display(metrics_output)\n",
    "display(plot_output)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    next_epoch = batch_per_epoch * (epoch + 1)\n",
    "        \n",
    "    avg_loss = AverageMeter()\n",
    "    for it, (X_batch, Xgt_batch) in zip(range(batch_per_epoch), train_loader):\n",
    "        net.train()\n",
    "        X_batch = X_batch.to(device)\n",
    "        Xgt_batch = Xgt_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        Xpred_batch, mean, logvar = net(X_batch)\n",
    "        main, kld = loss_function(Xpred_batch, Xgt_batch, mean, logvar)\n",
    "        loss = main + kld        \n",
    "        loss.backward()\n",
    "        \n",
    "        avg_loss.update(loss.item(), X_batch.shape[0])\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if pbar.n % 4 == 0:\n",
    "            with metrics_output:\n",
    "                metrics_output.clear_output(wait=True)\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"[E{epoch:04d}, {next_epoch}, {lr}] loss: \" +\n",
    "                      f\"{loss.item():10.5f} = {main.item():10.5f} + {kld.item():10.5f}\")\n",
    "            pbar.set_description(f'loss: {avg_loss}')\n",
    "        \n",
    "        if pbar.n % 128 == 0:\n",
    "            net.eval()            \n",
    "            with torch.no_grad():\n",
    "                img = []\n",
    "                for it, (X_batch, Xgt_batch) in zip(range(num_test_batches), test_loader):\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    mean, logvar = net.encode(X_batch)\n",
    "                    Xpred_batch = net.decode(mean).to('cpu')\n",
    "                    \n",
    "                    loc = [torch.cat(list(Xgt_batch), dim=2),\n",
    "                        torch.cat(list(Xpred_batch), dim=2)]\n",
    "                    for _ in range(num_sample):\n",
    "                        mean, logvar = net.encode(X_batch)\n",
    "                        mean = net.reparametrize(mean, logvar)\n",
    "                        Xpred_batch = net.decode(mean).to('cpu')\n",
    "                        loc.append(torch.cat(list(Xpred_batch), dim=2))\n",
    "                    loc = torch.cat(loc, dim=1)\n",
    "                    img.append(loc)\n",
    "                img = torch.cat(img, dim=1)\n",
    "            # save image\n",
    "            io.imsave(f'pics/{image_count}.jpg', (img.numpy()[0] * 255).astype(np.uint8))\n",
    "            image_count += 1\n",
    "            with plot_output:\n",
    "                plot_output.clear_output(wait=True)\n",
    "                _, ax = plt.subplots(1, 1, figsize=(16,24))\n",
    "                ax.imshow(T.ToPILImage()(img), cmap='gray')\n",
    "                ax.axis('off')\n",
    "                plt.show()\n",
    "        pbar.update(1)\n",
    "        # end of iteration\n",
    "    scheduler.step(epoch)\n",
    "    # end of epoch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('checkpoint.pth', 'wb') as f:\n",
    "    torch.save(net.to('cpu').state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
